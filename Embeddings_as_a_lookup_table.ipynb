{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import set_seed \n",
    "import nlpaug.augmenter.word as naw \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report \n",
    "from skmultilearn.problem_transform import BinaryRelevance \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "# set width for dataframe display\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "# dataset_url = \"https://git.io/nlp-with-transformers\"\n",
    "# df_issues = pd.read_json(dataset_url, lines=True)\n",
    "# print(f\"DataFrame shape: {df_issues.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save dataset with json format\n",
    "# df_issues_copy = df_issues.copy() \n",
    "# df_issues_copy.to_json(\"issues.json\", orient=\"records\", lines=True)\n",
    "# # load the data \n",
    "df_issues = pd.read_json(\"issues.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"url\", \"id\", \"title\", \"user\", \"labels\", \"state\", \"created_at\", \"body\"]\n",
    "df_issues.loc[2, cols].to_frame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all the columns\n",
    "print(df_issues.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issues[\"labels\"] = (df_issues[\"labels\"]\n",
    " .apply(lambda x: [meta[\"name\"] for meta in x]))\n",
    "df_issues[[\"labels\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts = df_issues[\"labels\"].explode().value_counts()\n",
    "print(f\"Number of labels: {len(df_counts)}\") \n",
    "df_counts.head(10).to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\"Core: Tokenization\": \"tokenization\",\n",
    " \"New model\": \"new model\",\n",
    " \"Core: Modeling\": \"model training\",\n",
    " \"Usage\": \"usage\",\n",
    " \"Core: Pipeline\": \"pipeline\",\n",
    " \"TensorFlow\": \"tensorflow or tf\",\n",
    " \"PyTorch\": \"pytorch\",\n",
    " \"Examples\": \"examples\",\n",
    " \"Documentation\": \"documentation\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_lables(x): \n",
    "    return [label_map[label] for label in x if label in label_map] \n",
    "df_issues[\"labels\"] = df_issues[\"labels\"].apply(filter_lables) \n",
    "all_labels = list(label_map.values()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts = df_issues[\"labels\"].explode().value_counts()\n",
    "print(f\"Number of labels: {len(df_counts)}\")\n",
    "df_counts.head(10).to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the unlabels data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issues[\"split\"] = \"unlabeled\"\n",
    "mark = df_issues[\"labels\"].apply(lambda x: len(x) > 0)\n",
    "df_issues.loc[mark, \"split\"] = \"labeled\"\n",
    "df_issues[\"split\"].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some examples\n",
    "for colume in ['title', 'body', 'labels']:\n",
    "    print(f\"{colume}: {df_issues[colume].iloc[26][:500]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat the title and body \n",
    "df_issues['text'] = (df_issues\n",
    "                     .apply(lambda x: f\"{x['title']} +  {x['body']}\", axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issues['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_before = len(df_issues)\n",
    "df_issues = df_issues.drop_duplicates(subset=['text'])\n",
    "print(f\"Remove {(len_before - len(df_issues)) / len_before:.2%} duplicates.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issues['text'].str.split().apply(len).hist(bins = np.linspace(0, 500, 50), grid=False, edgecolor='C0') \n",
    "plt.title(\"Words per issue\") \n",
    "plt.xlabel(\"Number of words\")\n",
    "plt.ylabel(\"Number of issues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create balance split by using sk ml\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([all_labels])\n",
    "mlb.transform([[\"tokenization\", \"new model\"], [\"pytorch\"]])  # example of target encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.model_selection import iterative_train_test_split \n",
    "def balanced_split(df , test_size = 0.5): \n",
    "    ind = np.expand_dims(np.arange(len(df)), axis=1)\n",
    "    labels = mlb.transform(df[\"labels\"]) # transform labels to binary\n",
    "    ind_train , _ , ind_test, _  = iterative_train_test_split(ind, labels, test_size=test_size) # split data\n",
    "    return df.iloc[ind_train[:, 0]], df.iloc[ind_test[:, 0]]  # return train and test DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of balance split\n",
    "![Alt text](image-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_clean = df_issues[[\"text\" , \"labels\", \"split\"]].reset_index(drop=True).copy() \n",
    "df_unsup = df_clean.loc[df_clean[\"split\"] == \"unlabeled\", [\"text\", \"labels\"]].copy()\n",
    "df_sup = df_clean.loc[df_clean[\"split\"] == \"labeled\", [\"text\", \"labels\"]].copy()\n",
    "\n",
    "np.random.seed(0)\n",
    " \n",
    "df_train , df_tmp = balanced_split(df_sup, test_size=0.5) \n",
    "df_valid , df_test = balanced_split(df_tmp, test_size=0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_unsup.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create datasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict \n",
    "ds = DatasetDict({\"train\": Dataset.from_pandas(df_train.reset_index(drop=True)),\n",
    "                  \"valid\": Dataset.from_pandas(df_valid.reset_index(drop=True)),\n",
    "                    \"test\": Dataset.from_pandas(df_test.reset_index(drop=True)),\n",
    "                    \"unsup\": Dataset.from_pandas(df_unsup.reset_index(drop=True))\n",
    "                  })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Training slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "all_indices = np.expand_dims(list(range(len(ds[\"train\"]))) , axis=1) \n",
    "print(f\"Number of samples: {len(all_indices)}\")\n",
    "indices_pool = all_indices \n",
    "lables = mlb.transform(ds[\"train\"][\"labels\"]) \n",
    "train_samples =  [8, 16, 32, 64, 128] \n",
    "train_slices , last_k = [], 0 \n",
    "for i , k in enumerate(train_samples):\n",
    "    # split off samples necessary to fill the gap to the next split size\n",
    "    indices_pool, lables, new_slice, _ = iterative_train_test_split(indices_pool, lables, (k - last_k)/len(lables))\n",
    "    last_k = k \n",
    "    if i == 0:\n",
    "        train_slices.append(new_slice)\n",
    "    else: train_slices.append(np.concatenate([train_slices[-1], new_slice]))\n",
    "# add full dataset as last slice \n",
    "train_slices.append(all_indices) , train_samples.append(len(ds[\"train\"]))\n",
    "train_slices = [np.squeeze(x) for x in train_slices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_slices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example data = {\n",
    "#     \"text\": [\"Text1\", \"Text2\", \"Text3\", \"Text4\", \"Text5\", \"Text6\", \"Text7\", \"Text8\", \"Text9\", \"Text10\"],\n",
    "#     \"labels\": [[\"A\"], [\"B\"], [\"A\", \"B\"], [\"A\"], [\"A\", \"B\"], [\"B\"], [\"A\"], [\"A\"], [\"B\"], [\"A\", \"B\"]],\n",
    "# }\n",
    "# Sample Sizes: [2, 4, 6, 10]\n",
    "# Train Slices: [array([4, 3], dtype=int64), array([4, 3, 8, 6], dtype=int64), array([4, 3, 8, 6, 1, 7], dtype=int64), array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Naive Bayesline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_labels(batch): \n",
    "    batch[\"label_ids\"] = mlb.transform(batch[\"labels\"])\n",
    "    return batch\n",
    "ds = ds.map(prepare_labels, batched=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# macro_scores , micro_scores = defaultdict(list), defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds[\"train\"].select([1,2,3])['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for train_slice in train_slices:\n",
    "#     # Get training slice and test data\n",
    "#     ds_train_sample = ds[\"train\"].select(train_slice)\n",
    "#     y_train = np.array(ds_train_sample[\"label_ids\"])\n",
    "#     y_test_labels = np.array(ds[\"test\"][\"label_ids\"])\n",
    "#     print(f\"Train size: {len(ds_train_sample)}\")\n",
    "\n",
    "#     # Use a simple count vectorizer to encode our texts as token counts\n",
    "#     count_vectorize = CountVectorizer()\n",
    "#     X_train_counts = count_vectorize.fit_transform(ds_train_sample[\"text\"])\n",
    "# #     print(\"shape of X_train_counts: \", X_train_counts.shape)\n",
    "# #     print(f\"shape of y_train {y_train.shape}\")\n",
    "#     X_test_counts = count_vectorize.transform(ds[\"test\"][\"text\"])\n",
    "# #     print(f\"Vocabulary size: {len(count_vectorize.vocabulary_)}\")   \n",
    "#     # Create and train our model!\n",
    "#     classifier = BinaryRelevance(classifier=MultinomialNB())\n",
    "#     classifier.fit(X_train_counts, y_train)\n",
    "#     print(\"Training done!\")\n",
    "#     # Generate predictions and evaluate\n",
    "#     y_pred_test = classifier.predict(X_test_counts)\n",
    "#     clf_report = classification_report(\n",
    "#             y_test_labels, y_pred_test, target_names=mlb.classes_, zero_division=0,\n",
    "#             output_dict=True)\n",
    "#     # Store metrics\n",
    "#     macro_scores[\"Naive Bayes\"].append(clf_report[\"macro avg\"][\"f1-score\"])\n",
    "#     micro_scores[\"Naive Bayes\"].append(clf_report[\"micro avg\"][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(micro_scores, macro_scores, sample_sizes, current_model):\n",
    "    fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
    "    for run in micro_scores.keys():\n",
    "        if run == current_model:\n",
    "            print(run)\n",
    "            ax0.plot(sample_sizes, micro_scores[run], label = run , linewidth=2)            \n",
    "            ax1.plot(sample_sizes, macro_scores[run], label = run , linewidth=2)\n",
    "        else:\n",
    "            print(run)\n",
    "            ax0.plot(sample_sizes, micro_scores[run], label=run,linestyle=\"dashed\")\n",
    "            ax1.plot(sample_sizes, macro_scores[run], label=run,linestyle=\"dashed\")\n",
    "    ax0.set_title(\"Micro F1 scores\")\n",
    "    ax1.set_title(\"Macro F1 scores\")\n",
    "    ax0.set_ylabel(\"Test set F1 score\")\n",
    "    ax0.legend(loc=\"lower right\")\n",
    "    for ax in [ax0, ax1]:\n",
    "        ax.set_xlabel(\"Number of training samples\")\n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.set_xticks(sample_sizes)\n",
    "        ax.set_xticklabels(sample_sizes)\n",
    "        ax.minorticks_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# plot_metrics(micro_scores, macro_scores, train_samples, \"Naive Bayes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with no labled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline \n",
    "# pipe = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "# movie_desc = \"The main characters of the movie madacascar \\\n",
    "# are a lion, a zebra, a giraffe, and a hippo. \"\n",
    "# prompt = \"The movie is about [MASK].\"\n",
    "# output = pipe(movie_desc + prompt, targets = ['animals', 'car']) \n",
    "# for element in output:\n",
    "#     print(f\"Token {element['token_str']}:\\t{element['score']:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie_desc = \"In the movie transformers aliens \\\n",
    "# can morph into a wide range of vehicles.\"\n",
    "# output = pipe(movie_desc + prompt, targets=[\"animals\", \"cars\"])\n",
    "# for element in output:\n",
    "#     print(f\"Token {element['token_str']}:\\t{element['score']:.3f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Multi-Genre NLI Corpus (MNLI) to zero short the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device  = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline \n",
    "# pipe = pipeline(\"zero-shot-classification\", device=device )\n",
    "# sample = ds[\"train\"][0]\n",
    "# output = pipe(sample[\"text\"], all_labels, multi_label=True)\n",
    "# print(output[\"sequence\"][:400])\n",
    "# print(\"\\nPredictions:\")\n",
    "# for label, score in zip(output[\"labels\"], output[\"scores\"]):\n",
    "#      print(f\"{label}, {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print pipe arguments\n",
    "# print(pipe.model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def zero_shot_pipeline(example): \n",
    "#     output = pipe(example[\"text\"], all_labels, multi_label=True)\n",
    "#     example[\"predictions\"] = output[\"labels\"]   \n",
    "#     example[\"scores\"] = output[\"scores\"] \n",
    "#     return example \n",
    "# # ds_zero_shot = ds[\"valid\"].map(zero_shot_pipeline) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_preds(example, threshold = None , topk = None ): \n",
    "#     '''Returns the predicted labels for a given example.'''\n",
    "#     preds = [] \n",
    "#     if threshold: \n",
    "#         for label, score in zip(example[\"predictions\"], example[\"scores\"]):\n",
    "#             if score >= threshold:\n",
    "#                 preds.append(label)\n",
    "#     elif topk: \n",
    "#         for i in range(topk):\n",
    "#             preds.append(example[\"predictions\"][i]) \n",
    "#     else: \n",
    "#         raise ValueError(\"Either threshold or topk must be set\") \n",
    "#     return {'pred_label_ids': list(np.squeeze(mlb.transform([preds])))}\n",
    "# def get_clf_report(ds):\n",
    "#     y_true = np.array(ds[\"label_ids\"])\n",
    "#     y_pred = np.array(ds[\"pred_label_ids\"]) \n",
    "#     return classification_report(y_true, y_pred, target_names=mlb.classes_, zero_division=0, output_dict=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot top-k pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# macros , micros = [] , []\n",
    "# topks = [1,2,3,4]\n",
    "# for topk in topks: \n",
    "#     ds_zero_shot = ds_zero_shot.map(get_preds, batched = False, fn_kwargs={\"topk\": topk}) \n",
    "#     clf_report = get_clf_report(ds_zero_shot) \n",
    "#     macros.append(clf_report[\"macro avg\"][\"f1-score\"]) \n",
    "#     micros.append(clf_report[\"micro avg\"][\"f1-score\"])\n",
    "\n",
    "# plt.plot(topks, micros, label=\"Micro F1 score\")\n",
    "# plt.plot(topks, macros, label=\"Macro F1 score\")\n",
    "# plt.xlabel(\"Top k predictions\")\n",
    "# plt.ylabel(\"F1 score\")\n",
    "# plt.legend(loc = 'best')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot threshold pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# macros , micros = [] , []\n",
    "# thresholds = np.linspace(0.01, 1, 100)\n",
    "# for threshold in thresholds: \n",
    "#     # dont print progress bar\n",
    "#     ds_zero_shot = ds_zero_shot.map(get_preds, batched = False, fn_kwargs={\"threshold\": threshold}) \n",
    "#     clf_report = get_clf_report(ds_zero_shot) \n",
    "#     macros.append(clf_report[\"macro avg\"][\"f1-score\"]) \n",
    "#     micros.append(clf_report[\"micro avg\"][\"f1-score\"])\n",
    "\n",
    "# plt.plot(thresholds, micros, label=\"Micro F1 score\")\n",
    "# plt.plot(thresholds, macros, label=\"Macro F1 score\")\n",
    "# plt.xlabel(\"Threshold predictions\")\n",
    "# plt.ylabel(\"F1 score\")\n",
    "# plt.legend(loc = 'best')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_t, best_micro = thresholds[np.argmax(micros)], np.max(micros)\n",
    "# print(f'Best threshold (micro): {best_t} with F1-score {best_micro:.2f}.')\n",
    "# best_t, best_macro = thresholds[np.argmax(macros)], np.max(macros)\n",
    "# print(f'Best threshold (micro): {best_t} with F1-score {best_macro:.2f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# micro_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# macro_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_zero_shot = ds['test'].map(zero_shot_pipeline)\n",
    "# ds_zero_shot = ds_zero_shot.map(get_preds, batched = False, fn_kwargs={\"topk\": 1})\n",
    "# clf_report = get_clf_report(ds_zero_shot)\n",
    "# for train_slice in train_slices: \n",
    "#     macro_scores[\"Zero-Shot\"].append(clf_report[\"macro avg\"][\"f1-score\"])\n",
    "#     micro_scores[\"Zero-Shot\"].append(clf_report[\"micro avg\"][\"f1-score\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_metrics(micro_scores, macro_scores, train_samples, \"Zero-Shot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# micro_scores.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_seed(3)\n",
    "# aug = naw.ContextualWordEmbsAug(\n",
    "#     model_path = 'distilbert-base-uncased',\n",
    "#     device = device, \n",
    "#     action = 'substitute' # thay the\n",
    "# )\n",
    "# text = 'Transformers are the most popular toys' \n",
    "# print(f\"Original: {text}\") \n",
    "# print(f\"Augmented Text: {aug.augment(text)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_train_sample_copy = ds_train_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def augment_text(batch, transformation_per_example = 10): \n",
    "#     \"\"\"return augment text and label ids\n",
    "\n",
    "#     Args:\n",
    "#         batch (_type_): batch of data\n",
    "#         transformation_per_example (int, optional):  Defaults to 1.\n",
    "\n",
    "#     Returns:\n",
    "#         dict: batch of data augmented\n",
    "#     \"\"\"\n",
    "#     text_aug , label_ids = [], [] \n",
    "#     for text, labels in zip(batch['text'], batch['label_ids']): \n",
    "#         text_aug += [text]\n",
    "#         label_ids += [labels] \n",
    "#         for _ in range(transformation_per_example):\n",
    "#             text_aug += aug.augment(text)\n",
    "#             label_ids += [labels]\n",
    "#     return {'text': text_aug, 'label_ids': label_ids}\n",
    "# # test function augment text\n",
    "# batch = {'text': ['Transformers are the most popular toys'], 'label_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n",
    "# output = augment_text(batch, transformation_per_example=3)\n",
    "# print(\"output: \", output) \n",
    "# print(\"type of output labels_ids: \", type(output['label_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_train_sample_copy = ds_train_sample_copy.map(\n",
    "#     augment_text, \n",
    "#     batched = True,\n",
    "#     remove_columns= ds_train_sample_copy.column_names,\n",
    "# ).shuffle(seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # iter through ds_train_sample_copy\n",
    "# dem = 0\n",
    "# for sample in ds_train_sample_copy:\n",
    "#     print(sample.keys())\n",
    "#     print(sample['text'][:100])\n",
    "#     print(sample['label_ids'][:100])\n",
    "#     dem += 1\n",
    "#     if dem == 2:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain naive bayes with augment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_train_sample_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_train_sample_copy.select([1,2,3])['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_train_sample_copy.select([1,2,3])['label_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(0)\n",
    "# all_indices = np.expand_dims(list(range(len(ds_train_sample_copy))) , axis=1) \n",
    "# print(f\"Number of samples: {len(all_indices)}\")\n",
    "# indices_pool = all_indices \n",
    "# lables = np.array(ds_train_sample_copy[\"label_ids\"]) \n",
    "# print(\"shape of lables: \", lables.shape)\n",
    "# train_samples = [8, 16, 32, 64, 128]\n",
    "# train_samples = [10* i for i in train_samples]\n",
    "# train_slices , last_k = [], 0 \n",
    "# for i , k in enumerate(train_samples):\n",
    "#     # split off samples necessary to fill the gap to the next split size\n",
    "#     indices_pool, lables, new_slice, _ = iterative_train_test_split(indices_pool, lables, (k - last_k)/len(lables))\n",
    "#     last_k = k     \n",
    "#     if i == 0:\n",
    "#         train_slices.append(new_slice)\n",
    "#     else: train_slices.append(np.concatenate([train_slices[-1], new_slice]))\n",
    "# # add full dataset as last slice \n",
    "# train_slices.append(all_indices) , train_samples.append(len(ds_train_sample_copy))\n",
    "# train_slices = [np.squeeze(x) for x in train_slices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reset Naive Bayes + Aug\n",
    "# macro_scores[\"Naive Bayes + Aug\"] = []\n",
    "# micro_scores[\"Naive Bayes + Aug\"] = []\n",
    "# for train_slice in train_slices:\n",
    "#     # Get training slice and test data\n",
    "#     # ds_train_sample_copy = ds[\"train\"].select(train_slice)\n",
    "#     ds_train_sample_copy_2 = ds_train_sample_copy.select(train_slice)\n",
    "#     y_train = np.array(ds_train_sample_copy_2[\"label_ids\"])\n",
    "#     y_test_labels = np.array(ds[\"test\"][\"label_ids\"])\n",
    "#     print(f\"Train size: {len(ds_train_sample_copy_2)}\")\n",
    "\n",
    "#     # Use a simple count vectorizer to encode our texts as token counts\n",
    "#     count_vectorize = CountVectorizer()\n",
    "#     X_train_counts = count_vectorize.fit_transform(ds_train_sample_copy_2[\"text\"])\n",
    "\n",
    "#     X_test_counts = count_vectorize.transform(ds[\"test\"][\"text\"])\n",
    "\n",
    "#     classifier = BinaryRelevance(classifier=MultinomialNB())\n",
    "#     classifier.fit(X_train_counts, y_train)\n",
    "#     print(\"Training done!\")\n",
    "#     # Generate predictions and evaluate\n",
    "#     y_pred_test = classifier.predict(X_test_counts)\n",
    "#     clf_report = classification_report(\n",
    "#             y_test_labels, y_pred_test, target_names=mlb.classes_, zero_division=0,\n",
    "#             output_dict=True)\n",
    "#     # Store metrics\n",
    "#     macro_scores[\"Naive Bayes + Aug\"].append(clf_report[\"macro avg\"][\"f1-score\"])\n",
    "#     micro_scores[\"Naive Bayes + Aug\"].append(clf_report[\"micro avg\"][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# macro_scores\n",
    "# defaultdict(list,\n",
    "#             {'Naive Bayes': [0.23288166214995487,\n",
    "#               0.21006897585844955,\n",
    "#               0.24086240556828795,\n",
    "#               0.25730500818220114,\n",
    "#               0.28271105113210376,\n",
    "#               0.27403710305671086],\n",
    "#              'Zero-Shot': [0.3869340923536515,\n",
    "#               0.3869340923536515,\n",
    "#               0.3869340923536515,\n",
    "#               0.3869340923536515,\n",
    "#               0.3869340923536515,\n",
    "#               0.3869340923536515],\n",
    "#              'Naive Bayes + Aug': [0.27235449735449735,\n",
    "#               0.278589655958077,\n",
    "#               0.33758862806420187,\n",
    "#               0.4415159953502743,\n",
    "#               0.4563280715286368,\n",
    "#               0.4832651770367019]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_samples = [8, 16, 32, 64, 128, 223]\n",
    "# plot_metrics(micro_scores, macro_scores, train_samples, \"Naive Bayes + Aug\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings_as_a_lookup_table.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import AutoTokenizer, AutoModel \n",
    "model_ckpt = \"microsoft/codebert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt).to(device)\n",
    "def mean_pooling(model_output, anttention_mask): \n",
    "    # Extract the token embedding \n",
    "    token_embeddings = model_output[0] # First element of model_output contains all token embeddings \n",
    "    # compute attention mask \n",
    "    input_mask_expanded = (anttention_mask\n",
    "                           .unsqueeze(-1)\n",
    "                           .expand(token_embeddings.size())\n",
    "                           .float()) \n",
    "    # Sum the embeddings , but ignore masked tokens\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) \n",
    "    sum_mark = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    # Return the average as a single vector\n",
    "    return sum_embeddings / sum_mark\n",
    "\n",
    "def embed_text(examples):\n",
    "    inputs = tokenizer(examples[\"text\"], padding=True, truncation=True, \n",
    "                       max_length=128, return_tensors=\"pt\",).to(device)\n",
    "    with torch.no_grad(): \n",
    "        model_output = model(**inputs)\n",
    "    pooled_embeds = mean_pooling(model_output, inputs[\"attention_mask\"])\n",
    "    return {\"embedding\": pooled_embeds.cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "embs_train = ds[\"train\"].map(embed_text, batched=True, batch_size=128)\n",
    "embs_valid = ds[\"valid\"].map(embed_text, batched=True, batch_size=128)\n",
    "embs_test = ds[\"test\"].map(embed_text, batched=True, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use FAISS library to embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_train.add_faiss_index(\"embedding\") # add faiss index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use FAISS library to search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i , k = 2 , 3 # Select the first query and 3 nearest neighbors\n",
    "rn , nl = \"\\r\\n\\r\\n\" , \"\\n\" # Used to remove newlines in text for compact display  \n",
    "\n",
    "query = np.array(embs_valid[i][\"embedding\"], dtype = np.float32) # query to search is embedding  \n",
    "scores, samples = embs_train.get_nearest_examples(\"embedding\", query, k=k)  # search from train data \n",
    "\n",
    "print(f\"QUERY LABELS: {embs_valid[i]['labels']}\") # print query labels (input)\n",
    "print(f\"QUERY TEXT:\\n{embs_valid[i]['text'][:200].replace(rn, nl)} [...]\\n\") # print query text (input)\n",
    "print(\"=\"*50)\n",
    "print(f\"Retrieved documents:\") # print retrieved documents\n",
    "for score, label, text in zip(scores, samples[\"labels\"], samples[\"text\"]):\n",
    "    print(\"=\"*50)\n",
    "    print(f\"TEXT:\\n{text[:200].replace(rn, nl)} [...]\")\n",
    "    print(f\"SCORE: {score:.2f}\")\n",
    "    print(f\"LABELS: {label}\")\n",
    "# all the text same label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the best value for k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_pred(sample, m): \n",
    "    return (np.sum(sample[\"label_ids\"], axis = 0) >= m ).astype(int) \n",
    "def find_best_k_m(ds_train, valid_queries, valid_labels, max_k = 17): \n",
    "    max_k = min(max_k, len(ds_train)) \n",
    "    perf_micro = np.zeros((max_k, max_k))\n",
    "    perf_macro = np.zeros((max_k, max_k))\n",
    "    for k in range(1, max_k): \n",
    "        for m in range(1 , k + 1 ):\n",
    "            _ , samples = ds_train.get_nearest_examples_batch(\"embedding\", valid_queries, k=k)\n",
    "            preds = np.array([get_sample_pred(sample, m) for sample in samples])\n",
    "            clf_report = classification_report(valid_labels, preds,\n",
    "                                               target_names=mlb.classes_, zero_division=0, output_dict=True)\n",
    "            perf_micro[k, m] = clf_report[\"micro avg\"][\"f1-score\"]\n",
    "            perf_macro[k, m] = clf_report[\"macro avg\"][\"f1-score\"]\n",
    "    return perf_micro, perf_macro "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_labbels = np.array(embs_valid[\"label_ids\"]) \n",
    "valid_queries = np.array(embs_valid[\"embedding\"], dtype = np.float32) \n",
    "perf_micro, perf_macro = find_best_k_m(embs_train, valid_queries, valid_labbels)\n",
    "\n",
    "fig , (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 4), sharey=True) \n",
    "ax0.imshow(perf_micro, cmap=\"Blues\")\n",
    "ax1.imshow(perf_macro, cmap=\"Blues\")\n",
    "\n",
    "ax0.set_title(perf_micro)\n",
    "ax1.imshow(perf_macro)\n",
    "\n",
    "ax0.set_title(\"Micro score\")\n",
    "ax0.set_ylabel(\"k\")\n",
    "ax1.set_title(\"macro scores\")\n",
    "for ax in [ax0,ax1]:\n",
    "    ax.set_xlim(0.5,17-0.5)\n",
    "    ax.set_ylim(17-0.5,0.5)\n",
    "    ax.set_xlabel(\"m\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k, m = np.unravel_index(perf_micro.argmax(), perf_micro.shape) # perf_micro shape = (k,m)\n",
    "print(f\"Best k: {k}, best m: {m}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init macro and micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import defaultdict\n",
    "from collections import defaultdict\n",
    "macro_scores = defaultdict(list,\n",
    "            {'Naive Bayes': [0.23288166214995487,\n",
    "              0.21006897585844955,\n",
    "              0.24086240556828795,\n",
    "              0.25730500818220114,\n",
    "              0.28271105113210376,\n",
    "              0.27403710305671086],\n",
    "             'Zero-Shot': [0.3869340923536515,\n",
    "              0.3869340923536515,\n",
    "              0.3869340923536515,\n",
    "              0.3869340923536515,\n",
    "              0.3869340923536515,\n",
    "              0.3869340923536515],\n",
    "             'Naive Bayes + Aug': [0.27235449735449735,\n",
    "              0.278589655958077,\n",
    "              0.33758862806420187,\n",
    "              0.4415159953502743,\n",
    "              0.4563280715286368,\n",
    "              0.4832651770367019],\n",
    "             'Embedding GPT2': [0.2359519022412816,\n",
    "              0.22399328684158407,\n",
    "              0.24991158684522524,\n",
    "              0.3304358431861561,\n",
    "              0.31501683091623417,\n",
    "              0.30677622391019166]})\n",
    "\n",
    "micro_scores = defaultdict(list,\n",
    "            {'Naive Bayes': [0.3604651162790698,\n",
    "              0.30208333333333337,\n",
    "              0.41081081081081083,\n",
    "              0.4435483870967742,\n",
    "              0.5046728971962616,\n",
    "              0.5346534653465347],\n",
    "             'Zero-Shot': [0.4444444444444445,\n",
    "              0.4444444444444445,\n",
    "              0.4444444444444445,\n",
    "              0.4444444444444445,\n",
    "              0.4444444444444445,\n",
    "              0.4444444444444445],\n",
    "             'Naive Bayes + Aug': [0.5081081081081081,\n",
    "              0.4954128440366973,\n",
    "              0.5084745762711865,\n",
    "              0.5679012345679012,\n",
    "              0.5590551181102362,\n",
    "              0.578125],\n",
    "             'Embedding GPT2': [0.3167155425219942,\n",
    "              0.36363636363636365,\n",
    "              0.39452054794520547,\n",
    "              0.5118483412322274,\n",
    "              0.46564885496183206,\n",
    "              0.5402843601895735]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print index of embs_train\n",
    "# embs_train.get_index(\"embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print len of train_slides \n",
    "for i in train_slices:\n",
    "    print(len(i))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of embs_train\n",
    "embs_train_copy = embs_train.map(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embs_train_copy.add_faiss_index(\"embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k_m = [(2,1), (7,2), (15,3), (3,2), (5,2), (14,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7ff6b3d2234692a0e08c32f488d0bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5499e9688798492dba65a98568e81bdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda228af73aa42de9cece89be1b1c1f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "macro_scores[\"Embedding\"] = []\n",
    "micro_scores[\"Embedding\"] = []\n",
    "# embs_train_copy.drop_index(\"embedding\") # remove faiss index\n",
    "test_labels = np.array(embs_test[\"label_ids\"])\n",
    "test_queries = np.array(embs_test[\"embedding\"], dtype = np.float32) \n",
    "\n",
    "for train_slice in train_slices:\n",
    "    # Craete a Faiss index from training slice\n",
    "    embs_train_tmp = embs_train_copy.select(train_slice) \n",
    "    embs_train_tmp.add_faiss_index(\"embedding\") \n",
    "    # get best k,m values with validation set \n",
    "    k , m = find_best_k_m(embs_train_tmp, valid_queries, valid_labbels)\n",
    "    k , m = np.unravel_index(k.argmax(), k.shape)\n",
    "    # Get predictions for test set\n",
    "    _ , samples = embs_train_tmp.get_nearest_examples_batch(\"embedding\", test_queries, k=int(k)) \n",
    "    preds = np.array([get_sample_pred(sample, m) for sample in samples]) \n",
    "    # Evaluate predictions\n",
    "    clf_report = classification_report(test_labels, preds, target_names=mlb.classes_, zero_division=0, output_dict=True) \n",
    "    macro_scores[\"Embedding\"].append(clf_report[\"macro avg\"][\"f1-score\"])\n",
    "    micro_scores[\"Embedding\"].append(clf_report[\"micro avg\"][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(micro_scores, macro_scores, train_samples, \"Embedding\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GEC_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
